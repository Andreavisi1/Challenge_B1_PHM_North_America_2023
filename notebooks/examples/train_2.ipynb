{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name                   object\n",
      "etichetta                   object\n",
      "health_level                 int64\n",
      "velocita                     int64\n",
      "torque                       int64\n",
      "rep                          int64\n",
      "sampling_rate                int64\n",
      "descrizione                 object\n",
      "duration                   float64\n",
      "num_samples                  int64\n",
      "horizontal_acceleration     object\n",
      "axial_acceleration          object\n",
      "vertical_acceleration       object\n",
      "tachometer_signal           object\n",
      "dtype: object\n",
      "=== PHASE 1: Feature Extraction ===\n",
      "Classe 0: 287 campioni, 41 features\n",
      "Classe 1: 295 campioni, 41 features\n",
      "Classe 2: 291 campioni, 41 features\n",
      "Classe 3: 267 campioni, 41 features\n",
      "Classe 4: 304 campioni, 41 features\n",
      "Classe 6: 276 campioni, 41 features\n",
      "Classe 8: 296 campioni, 41 features\n",
      "\n",
      "=== PHASE 2: Degradation Trend Analysis ===\n",
      "\n",
      "=== PHASE 3: Synthetic Data Generation ===\n",
      "\n",
      "Generando dati per classe 5...\n",
      "  → Generati 276 campioni sintetici\n",
      "\n",
      "Generando dati per classe 7...\n",
      "  → Generati 276 campioni sintetici\n",
      "\n",
      "Generando dati per classe 9...\n",
      "  → Generati 296 campioni sintetici\n",
      "\n",
      "Generando dati per classe 10...\n",
      "  → Generati 296 campioni sintetici\n",
      "\n",
      "=== PHASE 4: Validation ===\n",
      "Classe 5: Realismo=0.643\n",
      "Classe 7: Realismo=0.712\n",
      "Classe 9: Realismo=0.755\n",
      "Classe 10: Realismo=0.712\n",
      "\n",
      "=== PHASE 5: Data Integration ===\n",
      "\n",
      "Dataset finale: 3160 campioni, 41 features\n",
      "Distribuzione classi: {0: 287, 1: 295, 2: 291, 3: 267, 4: 304, 5: 276, 6: 276, 7: 276, 8: 296, 9: 296, 10: 296}\n",
      "features shape: (3160, 41)\n",
      "labels shape: (3160,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal, stats\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============ FEATURE EXTRACTION ============\n",
    "\n",
    "def extract_comprehensive_features(horizontal_acc, axial_acc, vertical_acc, tachometer, fs=20480):\n",
    "    \"\"\"\n",
    "    Estrae feature dal segnale triassiale + tachimetro.\n",
    "    Ritorna un dizionario {feature_name: value}.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # --- TIME & FREQ FEATURES per asse ---\n",
    "    for axis_name, signal_data in [('horizontal', horizontal_acc),\n",
    "                                   ('axial', axial_acc),\n",
    "                                   ('vertical', vertical_acc)]:\n",
    "        signal_data = np.asarray(signal_data).astype(float)\n",
    "\n",
    "        # Statistiche base\n",
    "        rms = np.sqrt(np.mean(signal_data**2)) if len(signal_data) else 0.0\n",
    "        std = np.std(signal_data) if len(signal_data) else 0.0\n",
    "        kurt = stats.kurtosis(signal_data) if len(signal_data) else 0.0\n",
    "        skew = stats.skew(signal_data) if len(signal_data) else 0.0\n",
    "        peak = float(np.max(np.abs(signal_data))) if len(signal_data) else 0.0\n",
    "\n",
    "        features[f'{axis_name}_rms'] = rms\n",
    "        features[f'{axis_name}_std'] = std\n",
    "        features[f'{axis_name}_kurtosis'] = kurt\n",
    "        features[f'{axis_name}_skewness'] = skew\n",
    "        features[f'{axis_name}_peak'] = peak\n",
    "        features[f'{axis_name}_crest_factor'] = (peak / rms) if rms > 1e-12 else 0.0\n",
    "        mean_abs = np.mean(np.abs(signal_data)) if len(signal_data) else 0.0\n",
    "        features[f'{axis_name}_shape_factor'] = (rms / mean_abs) if mean_abs > 1e-12 else 0.0\n",
    "        features[f'{axis_name}_impulse_factor'] = (peak / mean_abs) if mean_abs > 1e-12 else 0.0\n",
    "\n",
    "        # Energia in bande\n",
    "        if len(signal_data) >= 1024:\n",
    "            freqs, psd = welch(signal_data, fs=fs, nperseg=1024)\n",
    "            features[f'{axis_name}_energy_low']  = float(np.sum(psd[(freqs >= 10) & (freqs <= 100)]))\n",
    "            features[f'{axis_name}_energy_mid']  = float(np.sum(psd[(freqs >= 100) & (freqs <= 1000)]))\n",
    "            features[f'{axis_name}_energy_high'] = float(np.sum(psd[(freqs >= 1000) & (freqs <= 5000)]))\n",
    "        else:\n",
    "            features[f'{axis_name}_energy_low']  = 0.0\n",
    "            features[f'{axis_name}_energy_mid']  = 0.0\n",
    "            features[f'{axis_name}_energy_high'] = 0.0\n",
    "\n",
    "    # --- TACHOMETER FEATURES ---\n",
    "    rpm_estimation = extract_rpm_from_tachometer(np.asarray(tachometer).astype(float), fs)\n",
    "    features['rpm_mean'] = float(np.mean(rpm_estimation)) if len(rpm_estimation) else 0.0\n",
    "    features['rpm_std']  = float(np.std(rpm_estimation)) if len(rpm_estimation) else 0.0\n",
    "\n",
    "    # --- CROSS-AXIS CORRELATIONS ---\n",
    "    def _safe_corr(a, b):\n",
    "        a = np.asarray(a).astype(float); b = np.asarray(b).astype(float)\n",
    "        if len(a) < 2 or len(b) < 2: return 0.0\n",
    "        try:\n",
    "            return float(np.corrcoef(a, b)[0, 1])\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    features['corr_horiz_axial']    = _safe_corr(horizontal_acc, axial_acc)\n",
    "    features['corr_horiz_vertical'] = _safe_corr(horizontal_acc, vertical_acc)\n",
    "    features['corr_axial_vertical'] = _safe_corr(axial_acc, vertical_acc)\n",
    "\n",
    "    # --- COMPOSITE ---\n",
    "    ha = np.asarray(horizontal_acc).astype(float)\n",
    "    aa = np.asarray(axial_acc).astype(float)\n",
    "    va = np.asarray(vertical_acc).astype(float)\n",
    "    if len(ha) and len(aa) and len(va):\n",
    "        total_vibration = np.sqrt(ha**2 + aa**2 + va**2)\n",
    "        features['total_rms'] = float(np.sqrt(np.mean(total_vibration**2)))\n",
    "        features['total_kurtosis'] = float(stats.kurtosis(total_vibration))\n",
    "    else:\n",
    "        features['total_rms'] = 0.0\n",
    "        features['total_kurtosis'] = 0.0\n",
    "\n",
    "    axis_energies = [features['horizontal_rms'], features['axial_rms'], features['vertical_rms']]\n",
    "    denom = sum(axis_energies) + 1e-8\n",
    "    features['dominant_axis_ratio'] = float(max(axis_energies) / denom)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_rpm_from_tachometer(tachometer_signal, fs):\n",
    "    \"\"\"\n",
    "    Stima RPM dal tachimetro. Restituisce un array di RPM.\n",
    "    \"\"\"\n",
    "    t = np.asarray(tachometer_signal).astype(float)\n",
    "    if len(t) < 4 or np.allclose(np.max(np.abs(t)), 0.0):\n",
    "        return np.array([1000.0])\n",
    "\n",
    "    thresh = 0.5 * np.max(t)\n",
    "    peaks, _ = signal.find_peaks(t, height=thresh)\n",
    "    if len(peaks) < 2:\n",
    "        return np.array([1000.0])\n",
    "\n",
    "    intervals_s = np.diff(peaks) / float(fs)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        rpm_values = 60.0 / intervals_s\n",
    "        rpm_values = rpm_values[np.isfinite(rpm_values)]\n",
    "\n",
    "    if len(rpm_values) == 0:\n",
    "        return np.array([1000.0])\n",
    "\n",
    "    rpm_median = np.median(rpm_values)\n",
    "    rpm_filtered = rpm_values[np.abs(rpm_values - rpm_median) < 0.2 * rpm_median]\n",
    "    return rpm_filtered if len(rpm_filtered) > 0 else np.array([rpm_median])\n",
    "\n",
    "\n",
    "# ============ TREND & SYNTHETIC DATA ============\n",
    "\n",
    "def compute_degradation_trends(features_dict):\n",
    "    \"\"\"\n",
    "    features_dict: {class_id: list of feature vectors (consistent order)}\n",
    "    Ritorna trend per coppie di classi adiacenti.\n",
    "    \"\"\"\n",
    "    available_classes = sorted(features_dict.keys())\n",
    "    trends = {}\n",
    "\n",
    "    for i in range(len(available_classes) - 1):\n",
    "        c_low = available_classes[i]\n",
    "        c_up  = available_classes[i + 1]\n",
    "        lower = np.asarray(features_dict[c_low])\n",
    "        upper = np.asarray(features_dict[c_up])\n",
    "        if lower.size == 0 or upper.size == 0:\n",
    "            continue\n",
    "\n",
    "        n_feat = min(lower.shape[1], upper.shape[1])\n",
    "        trend = {}\n",
    "        for f in range(n_feat):\n",
    "            lower_mean = float(np.mean(lower[:, f]))\n",
    "            upper_mean = float(np.mean(upper[:, f]))\n",
    "            rate = (upper_mean - lower_mean) / max(1, (c_up - c_low))\n",
    "            trend[f] = {'rate': rate, 'lower_mean': lower_mean, 'upper_mean': upper_mean}\n",
    "\n",
    "        trends[f'{c_low}_to_{c_up}'] = trend\n",
    "\n",
    "    return trends\n",
    "\n",
    "\n",
    "def intelligent_interpolation(features_dict, target_class, degradation_trends):\n",
    "    \"\"\"\n",
    "    Interpolazione tra classi vicine con aggiustamento al trend.\n",
    "    \"\"\"\n",
    "    available = sorted(features_dict.keys())\n",
    "    lower_classes = [c for c in available if c < target_class]\n",
    "    upper_classes = [c for c in available if c > target_class]\n",
    "\n",
    "    if not lower_classes or not upper_classes:\n",
    "        return extrapolate_missing_class(features_dict, target_class, degradation_trends)\n",
    "\n",
    "    cl = max(lower_classes)\n",
    "    cu = min(upper_classes)\n",
    "\n",
    "    alpha = (target_class - cl) / max(1, (cu - cl))\n",
    "    lower = np.asarray(features_dict[cl])\n",
    "    upper = np.asarray(features_dict[cu])\n",
    "\n",
    "    if lower.size == 0 or upper.size == 0:\n",
    "        return np.empty((0, lower.shape[1] if lower.size else (upper.shape[1] if upper.size else 0)))\n",
    "\n",
    "    n = min(len(lower), len(upper))\n",
    "    n_feat = min(lower.shape[1], upper.shape[1])\n",
    "    synth = []\n",
    "\n",
    "    trend_key = f'{cl}_to_{cu}'\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        for f in range(n_feat):\n",
    "            base_interp = (1 - alpha) * lower[i, f] + alpha * upper[i, f]\n",
    "            if trend_key in degradation_trends and f in degradation_trends[trend_key]:\n",
    "                tinfo = degradation_trends[trend_key][f]\n",
    "                expected = tinfo['lower_mean'] + tinfo['rate'] * (target_class - cl)\n",
    "                row.append(0.7 * base_interp + 0.3 * expected)\n",
    "            else:\n",
    "                row.append(base_interp)\n",
    "        synth.append(row)\n",
    "\n",
    "    return np.asarray(synth)\n",
    "\n",
    "\n",
    "def extrapolate_missing_class(features_dict, target_class, degradation_trends):\n",
    "    \"\"\"\n",
    "    Estrapolazione conservativa per classi estreme.\n",
    "    \"\"\"\n",
    "    available = sorted(features_dict.keys())\n",
    "    if len(available) == 0:\n",
    "        return np.empty((0, 0))\n",
    "\n",
    "    if target_class > max(available) and len(available) >= 2:\n",
    "        ref = available[-2:]\n",
    "        trend_key = f'{ref[0]}_to_{ref[1]}'\n",
    "        base = np.asarray(features_dict[ref[1]])\n",
    "        if base.size and trend_key in degradation_trends:\n",
    "            out = []\n",
    "            for sample in base:\n",
    "                row = []\n",
    "                for f in range(len(sample)):\n",
    "                    tinfo = degradation_trends[trend_key].get(f, {'rate': 0.0})\n",
    "                    extrapolation_factor = target_class - ref[1]\n",
    "                    damping = np.exp(-0.1 * extrapolation_factor)\n",
    "                    row.append(sample[f] + tinfo['rate'] * extrapolation_factor * damping)\n",
    "                out.append(row)\n",
    "            return np.asarray(out)\n",
    "\n",
    "    # fallback: duplica la classe più vicina + rumore relativo\n",
    "    closest = min(available, key=lambda x: abs(x - target_class))\n",
    "    base = np.asarray(features_dict[closest])\n",
    "    if base.size == 0:\n",
    "        return np.empty((0, 0))\n",
    "    noise_scale = 0.1 * abs(target_class - closest)\n",
    "    noise = np.random.normal(0, noise_scale, base.shape)\n",
    "    return base + base * noise\n",
    "\n",
    "\n",
    "def add_physics_informed_noise(synthetic_data, reference_features, target_class, feature_names):\n",
    "    \"\"\"\n",
    "    Aggiunge rumore realistico e clippa feature non-negative.\n",
    "    - synthetic_data: (n_synth, n_feat)\n",
    "    - reference_features: list/array di feature reali per stimare la scala del rumore\n",
    "    - feature_names: lista di nomi in ordine (serve per i vincoli fisici)\n",
    "    \"\"\"\n",
    "    if synthetic_data.size == 0:\n",
    "        return synthetic_data\n",
    "\n",
    "    reference_array = np.asarray(reference_features)\n",
    "    if reference_array.size == 0:\n",
    "        # fallback: rumore fisso\n",
    "        noise_profile = np.ones(synthetic_data.shape[1]) * 0.1\n",
    "    else:\n",
    "        noise_profile = np.std(reference_array, axis=0)\n",
    "        noise_profile = np.where(np.isfinite(noise_profile), noise_profile, 0.0)\n",
    "\n",
    "    degradation_factor = target_class / 10.0  # 0..1\n",
    "    adaptive_noise_scale = 0.05 + 0.15 * degradation_factor\n",
    "\n",
    "    noisy = []\n",
    "    for sample in synthetic_data:\n",
    "        noise = np.random.normal(0, adaptive_noise_scale * noise_profile)\n",
    "        ns = sample + noise\n",
    "\n",
    "        # vincoli fisici per feature >= 0\n",
    "        nonneg_tokens = ('rms', 'energy', 'std', 'peak')\n",
    "        for i, fname in enumerate(feature_names):\n",
    "            if any(tok in fname for tok in nonneg_tokens):\n",
    "                ns[i] = max(1e-6, ns[i])\n",
    "\n",
    "        noisy.append(ns)\n",
    "\n",
    "    return np.asarray(noisy)\n",
    "\n",
    "\n",
    "def validate_synthetic_realism(real_features_dict, synthetic_features_dict):\n",
    "    \"\"\"\n",
    "    Stima un punteggio di 'realismo' confrontando distribuzioni (KS test) con classi vicine.\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    for missing_class, synth in synthetic_features_dict.items():\n",
    "        synth = np.asarray(synth)\n",
    "        if synth.size == 0:\n",
    "            validation_results[missing_class] = {'realism_score': 0.0, 'n_samples': 0, 'feature_consistency': False}\n",
    "            continue\n",
    "\n",
    "        available_classes = list(real_features_dict.keys())\n",
    "        if not available_classes:\n",
    "            validation_results[missing_class] = {'realism_score': 0.0, 'n_samples': len(synth), 'feature_consistency': False}\n",
    "            continue\n",
    "\n",
    "        closest = sorted(available_classes, key=lambda x: abs(x - missing_class))[:2]\n",
    "        accum = 0.0\n",
    "        feat_var_list = []\n",
    "\n",
    "        for cc in closest:\n",
    "            real = np.asarray(real_features_dict[cc])\n",
    "            if real.size == 0:\n",
    "                continue\n",
    "            n_feat = min(synth.shape[1], real.shape[1])\n",
    "            sims = []\n",
    "            for f in range(n_feat):\n",
    "                ks_stat, _ = stats.ks_2samp(synth[:, f], real[:, f])\n",
    "                sims.append(1.0 - float(ks_stat))\n",
    "            if sims:\n",
    "                accum += float(np.mean(sims))\n",
    "                feat_var_list.extend(sims)\n",
    "\n",
    "        if not feat_var_list:\n",
    "            realism = 0.0\n",
    "            consistency = False\n",
    "        else:\n",
    "            realism = accum / max(1, len(closest))\n",
    "            consistency = (np.std(feat_var_list) < 0.3)\n",
    "\n",
    "        validation_results[missing_class] = {\n",
    "            'realism_score': float(realism),\n",
    "            'n_samples': int(len(synth)),\n",
    "            'feature_consistency': bool(consistency)\n",
    "        }\n",
    "        print(f\"Classe {missing_class}: Realismo={validation_results[missing_class]['realism_score']:.3f}\")\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "\n",
    "# ============ STRATEGIA COMPLETA ============\n",
    "\n",
    "def enhanced_missing_class_strategy_from_dataframe(df, label_col='label',\n",
    "                                                   h_col='horizontal_acceleration',\n",
    "                                                   a_col='axial_acceleration',\n",
    "                                                   v_col='vertical_acceleration',\n",
    "                                                   t_col='tachometer_signal',\n",
    "                                                   fs=20480,\n",
    "                                                   missing_classes=(5, 7, 9, 10)):\n",
    "    \"\"\"\n",
    "    Pipeline completa a partire da un DataFrame.\n",
    "    Ritorna: dict con features (scaled), labels, scaler, feature_names, synthetic_validation, degradation_trends\n",
    "    \"\"\"\n",
    "    # 1) Prepara feature_names UNA VOLTA, per mantenere l'ordine\n",
    "    feature_names = sorted(extract_comprehensive_features(\n",
    "        np.zeros(1024), np.zeros(1024), np.zeros(1024), np.zeros(1024), fs=fs\n",
    "    ).keys())\n",
    "\n",
    "    # 2) Estrazione feature reali per classe\n",
    "    print(\"=== PHASE 1: Feature Extraction ===\")\n",
    "    features_by_class = {}\n",
    "    counts_by_class = {}\n",
    "\n",
    "    for class_id, group in df.groupby(label_col):\n",
    "        class_features = []\n",
    "        for _, row in group.iterrows():\n",
    "            sample_features = extract_comprehensive_features(\n",
    "                horizontal_acc=row[h_col],\n",
    "                axial_acc=row[a_col],\n",
    "                vertical_acc=row[v_col],\n",
    "                tachometer=row[t_col],\n",
    "                fs=fs\n",
    "            )\n",
    "            # vettore in ordine fisso\n",
    "            vec = [sample_features[k] for k in feature_names]\n",
    "            class_features.append(vec)\n",
    "\n",
    "        features_by_class[int(class_id)] = class_features\n",
    "        counts_by_class[int(class_id)] = len(class_features)\n",
    "        n_feat = len(feature_names) if class_features else 0\n",
    "        print(f\"Classe {class_id}: {len(class_features)} campioni, {n_feat} features\")\n",
    "\n",
    "    # 3) Trend\n",
    "    print(\"\\n=== PHASE 2: Degradation Trend Analysis ===\")\n",
    "    degradation_trends = compute_degradation_trends(features_by_class)\n",
    "\n",
    "    # 4) Dati sintetici per classi mancanti\n",
    "    print(\"\\n=== PHASE 3: Synthetic Data Generation ===\")\n",
    "    synthetic_features = {}\n",
    "\n",
    "    for mc in missing_classes:\n",
    "        print(f\"\\nGenerando dati per classe {mc}...\")\n",
    "        base = intelligent_interpolation(features_by_class, mc, degradation_trends)\n",
    "        # scegli la classe reale più vicina come riferimento per il rumore\n",
    "        if features_by_class:\n",
    "            closest_real = min(features_by_class.keys(), key=lambda x: abs(x - mc))\n",
    "            ref_feats = features_by_class[closest_real]\n",
    "        else:\n",
    "            ref_feats = []\n",
    "        noisy = add_physics_informed_noise(base, ref_feats, mc, feature_names)\n",
    "        synthetic_features[int(mc)] = noisy\n",
    "        print(f\"  → Generati {len(noisy)} campioni sintetici\")\n",
    "\n",
    "    # 5) Validazione\n",
    "    print(\"\\n=== PHASE 4: Validation ===\")\n",
    "    validation_results = validate_synthetic_realism(features_by_class, synthetic_features)\n",
    "\n",
    "    # 6) Integrazione\n",
    "    print(\"\\n=== PHASE 5: Data Integration ===\")\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for cid, feats in features_by_class.items():\n",
    "        all_features.extend(feats)\n",
    "        all_labels.extend([cid] * len(feats))\n",
    "\n",
    "    for cid, feats in synthetic_features.items():\n",
    "        if feats.size:\n",
    "            all_features.extend(feats.tolist())\n",
    "            all_labels.extend([cid] * len(feats))\n",
    "\n",
    "    all_features = np.asarray(all_features, dtype=float)\n",
    "    all_labels = np.asarray(all_labels, dtype=int)\n",
    "\n",
    "    if all_features.size == 0:\n",
    "        raise ValueError(\"Nessuna feature disponibile dopo integrazione (reali + sintetiche).\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    all_features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "    print(f\"\\nDataset finale: {all_features.shape[0]} campioni, {all_features.shape[1]} features\")\n",
    "    unique, counts = np.unique(all_labels, return_counts=True)\n",
    "    print(f\"Distribuzione classi: {dict(zip(unique.tolist(), counts.tolist()))}\")\n",
    "\n",
    "    return {\n",
    "        'features': all_features_scaled,\n",
    "        'labels': all_labels,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'synthetic_validation': validation_results,\n",
    "        'degradation_trends': degradation_trends\n",
    "    }\n",
    "\n",
    "\n",
    "# ============ ESEMPIO D'USO ============\n",
    "\n",
    "# Carica il tuo pickle\n",
    "train_data = pd.read_pickle('../data/processed/train_data_dowsampled_walt.pkl')\n",
    "print(train_data.dtypes)\n",
    "\n",
    "result = enhanced_missing_class_strategy_from_dataframe(\n",
    "    train_data,\n",
    "    label_col='health_level',\n",
    "    h_col='horizontal_acceleration',\n",
    "    a_col='axial_acceleration',\n",
    "    v_col='vertical_acceleration',\n",
    "    t_col='tachometer_signal',\n",
    "    fs=20480,\n",
    "    missing_classes=(5, 7, 9, 10)  # modifica se serve\n",
    ")\n",
    "\n",
    "features = result['features']\n",
    "labels = result['labels']\n",
    "\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27f05537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1236 - loss: 2.3550 - val_accuracy: 0.1818 - val_loss: 2.2950\n",
      "Epoch 2/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.1815 - loss: 2.2669 - val_accuracy: 0.2075 - val_loss: 2.2163\n",
      "Epoch 3/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.2028 - loss: 2.2032 - val_accuracy: 0.2154 - val_loss: 2.1704\n",
      "Epoch 4/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.2166 - loss: 2.1313 - val_accuracy: 0.2352 - val_loss: 2.0697\n",
      "Epoch 5/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.2537 - loss: 2.0532 - val_accuracy: 0.2510 - val_loss: 2.0279\n",
      "Epoch 6/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.2671 - loss: 1.9928 - val_accuracy: 0.3063 - val_loss: 1.9454\n",
      "Epoch 7/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.2829 - loss: 1.9428 - val_accuracy: 0.2964 - val_loss: 1.9387\n",
      "Epoch 8/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.3180 - loss: 1.8792 - val_accuracy: 0.3320 - val_loss: 1.8110\n",
      "Epoch 9/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3299 - loss: 1.8330 - val_accuracy: 0.2905 - val_loss: 1.8302\n",
      "Epoch 10/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.3368 - loss: 1.7960 - val_accuracy: 0.3715 - val_loss: 1.7377\n",
      "Epoch 11/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.3581 - loss: 1.7480 - val_accuracy: 0.3577 - val_loss: 1.7149\n",
      "Epoch 12/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3665 - loss: 1.7080 - val_accuracy: 0.3241 - val_loss: 1.6717\n",
      "Epoch 13/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3778 - loss: 1.6725 - val_accuracy: 0.3439 - val_loss: 1.6269\n",
      "Epoch 14/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3833 - loss: 1.6445 - val_accuracy: 0.3577 - val_loss: 1.6055\n",
      "Epoch 15/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3922 - loss: 1.6124 - val_accuracy: 0.3794 - val_loss: 1.5725\n",
      "Epoch 16/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3813 - loss: 1.6010 - val_accuracy: 0.4032 - val_loss: 1.6199\n",
      "Epoch 17/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4036 - loss: 1.5813 - val_accuracy: 0.3340 - val_loss: 1.5594\n",
      "Epoch 18/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4001 - loss: 1.5553 - val_accuracy: 0.3656 - val_loss: 1.5394\n",
      "Epoch 19/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4075 - loss: 1.5423 - val_accuracy: 0.3874 - val_loss: 1.5334\n",
      "Epoch 20/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4159 - loss: 1.5098 - val_accuracy: 0.3636 - val_loss: 1.5205\n",
      "Epoch 21/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4233 - loss: 1.4878 - val_accuracy: 0.3735 - val_loss: 1.5168\n",
      "Epoch 22/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4293 - loss: 1.4774 - val_accuracy: 0.3874 - val_loss: 1.4901\n",
      "Epoch 23/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4446 - loss: 1.4568 - val_accuracy: 0.3715 - val_loss: 1.4772\n",
      "Epoch 24/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4258 - loss: 1.4437 - val_accuracy: 0.3893 - val_loss: 1.4487\n",
      "Epoch 25/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4382 - loss: 1.4309 - val_accuracy: 0.3992 - val_loss: 1.4443\n",
      "Epoch 26/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4387 - loss: 1.4293 - val_accuracy: 0.4032 - val_loss: 1.4736\n",
      "Epoch 27/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4421 - loss: 1.4027 - val_accuracy: 0.3715 - val_loss: 1.4386\n",
      "Epoch 28/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4505 - loss: 1.3912 - val_accuracy: 0.4071 - val_loss: 1.4279\n",
      "Epoch 29/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4431 - loss: 1.3982 - val_accuracy: 0.4190 - val_loss: 1.4169\n",
      "Epoch 30/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4530 - loss: 1.3626 - val_accuracy: 0.4130 - val_loss: 1.4141\n",
      "Epoch 31/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4629 - loss: 1.3410 - val_accuracy: 0.3953 - val_loss: 1.3951\n",
      "Epoch 32/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4550 - loss: 1.3515 - val_accuracy: 0.4190 - val_loss: 1.4014\n",
      "Epoch 33/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4773 - loss: 1.3166 - val_accuracy: 0.3972 - val_loss: 1.4080\n",
      "Epoch 34/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4777 - loss: 1.3177 - val_accuracy: 0.4051 - val_loss: 1.3971\n",
      "Epoch 35/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.4674 - loss: 1.3202 - val_accuracy: 0.4328 - val_loss: 1.3649\n",
      "Epoch 36/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4758 - loss: 1.3127 - val_accuracy: 0.4229 - val_loss: 1.3752\n",
      "Epoch 37/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4822 - loss: 1.3070 - val_accuracy: 0.4368 - val_loss: 1.3901\n",
      "Epoch 38/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4985 - loss: 1.2644 - val_accuracy: 0.4427 - val_loss: 1.3352\n",
      "Epoch 39/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4871 - loss: 1.2704 - val_accuracy: 0.4209 - val_loss: 1.3391\n",
      "Epoch 40/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4990 - loss: 1.2461 - val_accuracy: 0.4466 - val_loss: 1.2995\n",
      "Epoch 41/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5069 - loss: 1.2343 - val_accuracy: 0.4269 - val_loss: 1.3653\n",
      "Epoch 42/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5045 - loss: 1.2271 - val_accuracy: 0.4407 - val_loss: 1.2961\n",
      "Epoch 43/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4960 - loss: 1.2361 - val_accuracy: 0.4506 - val_loss: 1.3039\n",
      "Epoch 44/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5129 - loss: 1.2059 - val_accuracy: 0.4545 - val_loss: 1.3309\n",
      "Epoch 45/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5168 - loss: 1.2001 - val_accuracy: 0.4565 - val_loss: 1.2829\n",
      "Epoch 46/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5104 - loss: 1.2000 - val_accuracy: 0.4565 - val_loss: 1.2683\n",
      "Epoch 47/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5356 - loss: 1.1904 - val_accuracy: 0.4506 - val_loss: 1.3178\n",
      "Epoch 48/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5198 - loss: 1.1795 - val_accuracy: 0.4466 - val_loss: 1.2814\n",
      "Epoch 49/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5307 - loss: 1.1952 - val_accuracy: 0.4565 - val_loss: 1.2556\n",
      "Epoch 50/50\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5252 - loss: 1.1895 - val_accuracy: 0.4565 - val_loss: 1.2486\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.55      0.49        58\n",
      "           1       0.47      0.58      0.52        59\n",
      "           2       0.43      0.38      0.40        58\n",
      "           3       0.52      0.24      0.33        54\n",
      "           4       0.61      0.59      0.60        61\n",
      "           5       0.83      0.71      0.76        55\n",
      "           6       0.47      0.45      0.46        55\n",
      "           7       0.66      0.78      0.72        55\n",
      "           8       0.26      0.31      0.28        59\n",
      "           9       0.27      0.07      0.11        59\n",
      "          10       0.42      0.75      0.54        59\n",
      "\n",
      "    accuracy                           0.49       632\n",
      "   macro avg       0.49      0.49      0.47       632\n",
      "weighted avg       0.49      0.49      0.47       632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ================= PREPARA DATI =================\n",
    "X = result[\"features\"]                      # (N, F) float\n",
    "y = result[\"labels\"]                        # (N,) int o categ\n",
    "if y.dtype != np.int32 and y.dtype != np.int64:\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Aggiungi dimensione canale per Conv1D: (N, F) -> (N, F, 1)\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ================= MODELL0 =================\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(32, 7, padding='same', activation='relu', input_shape=(n_features, 1)),\n",
    "    tf.keras.layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ================= TRAINING =================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================= VALUTAZIONE =================\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
